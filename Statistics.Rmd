---
title: "Statistics"
author: "Michael Yoon"
date: "Jan 21, 2023"
output: github_document
---

## Basics

#### Comparison between population and sample mean
```{r}
#Get data
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename <- "femaleControlsPopulation.csv"
if (!file.exists(filename)) download(url,destfile=filename)
population <- read.csv(filename)
population <- unlist(population) # turn it into a numeric

#Get samples of size n
n <- 10000
null <- vector("numeric",n)
for (i in 1:n) {
  control <- sample(population,12)
  treatment <- sample(population,12)
  null[i] <- mean(treatment) - mean(control)
}

obsdiff <- mean(treatment) - mean(control)
mean(null >= obsdiff)
```

#### Confidence interval and error

Adapted from: https://www.huber.embl.de/msmb/06-chap.html#sec-cointossing

Highlighted graph shows rejection region indicating range of values to reject the null hypothesis. If the bias concentrates towards both tails or one it is either a two-sided or one-sided test, respectively. 

For error, we have 4 options:
1) Reject null hypothesis: Null hypothesis is true = Type 1 error (FP), Null hypothesis is false = True positive
2) Do not reject null hypothesis: Null hypothesis is true = True negative, Null hypothesis is false = Type 2 error (FN)
Decreasing the false positive rate will make he data more conservative, but we increase the false negative rate and vice-versa. FPR is the same as alpha. 1 - alpha = specifity of test and 1 - Beta (power) = sensitivity or true positive rate of test.

```{r}
library(tidyverse)
set.seed(0xdada)
numFlips = 100
probHead = 0.6
coinFlips = sample(c("H", "T"), size = numFlips,
  replace = TRUE, prob = c(probHead, 1 - probHead))
head(coinFlips)
k = 0:numFlips
numHeads = sum(coinFlips == "H")
binomDensity = tibble(k = k,
     p = dbinom(k, size = numFlips, prob = 0.5))

#Plot simulations of coin toss
numSimulations = 10000
outcome = replicate(numSimulations, {
  coinFlips = sample(c("H", "T"), size = numFlips,
                     replace = TRUE, prob = c(0.5, 0.5))
  sum(coinFlips == "H")
})
ggplot(tibble(outcome)) + xlim(-0.5, 100.5) +
  geom_histogram(aes(x = outcome), binwidth = 1, center = 50) +
  geom_vline(xintercept = numHeads, col = "blue")

#Comamand to do flips
binom.test(x = numHeads, n = numFlips, p = 0.5)

#Plot coin simulation with false positive rate = 0.05
alpha = 0.05
binomDensity = arrange(binomDensity, p) |>
        mutate(reject = (cumsum(p) <= alpha))

ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p, col = reject), stat = "identity") +
  scale_colour_manual(
    values = c(`TRUE` = "red", `FALSE` = "darkgrey")) +
  geom_vline(xintercept = numHeads, col = "blue") +
  theme(legend.position = "none")
```

## Normal (Gaussian) Distribution

Mathemathical function is stored as pnorm function in R. Normal distributions follow an empirical rule where one std dev away from mean = 68% of observations, two std dev away from mean = 95% of observations, and three std dev away from mean = 99.7% of observations. 

## Probability Desnity Function

Representation of probability vs x-values where the chance of getting a range of values can be found by integrating under the distribution curve.

## T test

The t-test determines whether there is a difference between means of two groups following a t-distribution with n1 + n2 - 2 degrees of freedom. 

Assumes the data is independent, comes from a normal distribution, with both sets having similar variance. Comes in many variations where it could be sided (reject the null if the value is too large or small), two-sampled meaning that the groups where compared against each other and not another fixed value, and unpaired meaning there was no 1:1 mapping between the values of the two groups (paired = before and after timepoints).

Resulting t-values show how many standard deviations apart the difference is. The value is a random variable by the random sample which follows a t-distribution (assuming data is normal) which is only dependent on the degrees of freedom calculate by n subtract 1/2 based on one-side or two-side. Because it follows this probability distribution, we can calculate the p-value of seeing the test statistic (t-value) = probability of seeing a value higher or lower than the positive/negative of the t-value.

Sample size n (20-30) higher gives more significant results. Even if the original distribution is not normal, due to CLT it will become normal. Independence means that measurements must not be connected (duplicates/replicates).

```{r}
library("ggbeeswarm")
data("PlantGrowth")
ggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +
  geom_beeswarm() + theme(legend.position = "none")
tt = with(PlantGrowth,
          t.test(weight[group =="ctrl"],
                 weight[group =="trt2"],
                 var.equal = TRUE))
tt

#Independence test
with(rbind(PlantGrowth, PlantGrowth),
       t.test(weight[group == "ctrl"],
              weight[group == "trt2"],
              var.equal = TRUE))
```

## Linear Regression

In terms of the output lm vs t-test, lm (intercept) represents sample mean of the WT group. The main difference is that the t-test compares the means between two groups, but linear regression is trying to find the difference between the two means. Anova is also a type of linear regression except with multiple variables.

Analysis: Call shows formula, residuals showing quantiles (want median to be close to zero), coefficients show values for formula (intercept is y-int, weight is coefficient front of x, std error of estimates and corresponding t-value to show how p-value is calculated (represents the distance b/w pred regression values against observed values). Multiple and adjusted R2 values represent how much the variable explains the variation e.g. weight (x) explains 61% of variation in the size (y). Adjusted takes into account other parameters. F-statistic represents whether the R2 is significant or not.
```{r}
#Load data
mouse.data <- data.frame(weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3), size = c(1.4, 2.6, 1, 3.7, 5.5, 3.2, 3, 4.9, 6.3))
mouse.data

mouse.regresison <- lm(size ~ weight, data = mouse.data)
summary(mouse.regresison)
```
## Multiple Linear Regression

Based on class notes: Linear regression is limited to the difference between two means during two timepoints. Statements to make:

Basic example: 

f1 is control and high temp while f2 is A(controlN), B(midN), C(highN)
So the output shows
1) Intercept - plant pred to have length of 0.98 cm under control temp and control nitrogen cond
2) f1High - plant pred to grow 3 cm under high temp and control nitrogen conditions
...
5) f1High:f2B - effect of medium nitrogen in high temp increase by 1 compared to effect of medium nitrogen under control temp
```{r}
library(tidyverse)
# interpreting interaction coefficients from lm first case two categorical
# variables
set.seed(12)
f1 <- gl(n = 2, k = 30, labels = c("Low", "High"))
f2 <- as.factor(rep(c("A", "B", "C"), times = 20))
modmat <- model.matrix(~f1 * f2, data.frame(f1 = f1, f2 = f2))
coeff <- c(1, 3, -2, -4, 1, -1.2)
y <- rnorm(n = 60, mean = modmat %*% coeff, sd = 0.1)
dat <- data.frame(y = y, f1 = f1, f2 = f2)
summary(lm(y ~ f1 * f2))

# second case one categorical and one continuous variable
x <- runif(50, 0, 10)
f1 <- gl(n = 2, k = 25, labels = c("Low", "High"))
modmat <- model.matrix(~x * f1, data.frame(f1 = f1, x = x))
coeff <- c(1, 3, -2, 1.5)
y <- rnorm(n = 50, mean = modmat %*% coeff, sd = 0.5)
dat <- data.frame(y = y, f1 = f1, x = x)
summary(lm(y ~ x * f1))

# plot (easy one)
ggplot(dat, aes(x = x, y = y, color = f1)) + geom_point() + geom_abline(xintercept = 1.17, 
    slope = 2.98, color = "red") + geom_abline(xintercept = 1.17 - 2.09, slope = 2.98 + 
    1.5, color = "blue") + geom_text(label = "y = 1.17 + 2.98 * x", x = 5, y = 10, 
    angle = 30, color = "black", size = 8) + geom_text(label = "y = (1.17 - 2.09) + (2.98 + 1.5) * x", 
    x = 5, y = 25, angle = 45, color = "black", size = 8)
```



## ANOVA

Purpose: One-ways are designed to test equality across multiple means while two-ways are designed to test relationship between two independent variables on a dependent variable. Used to measure overall association between response and factor, the weighted average. 

## Principal Coordinate Analysis

Purpose: To observe major sources of variation from variables, visualize trends or patterns in the form of clustering, and to see if those variations are associated with a biological condition.

Results: We see that from plotIndiv that gene expression is altered and clustered in response to different dosage levels of drug (50/150), but in addition, we can see that time of exposure also plays a role.

```{r}
library(mixOmics)
data(liver.toxicity)
X <- liver.toxicity$gene

#Default parameters (center = TRUE so mean = 0, ncomp = 2 so first two PCA values capturing highest var, scale = FALSE so data is not scaled)
MyResult.pca <- pca(X)     # 1 Run the method
plotIndiv(MyResult.pca, ind.names = FALSE,
          group = liver.toxicity$treatment$Dose.Group,
          pch = as.factor(liver.toxicity$treatment$Time.Group),
          legend = TRUE, title = 'Liver toxicity: genes, PCA comp 1 - 2',
          legend.title = 'Dose', legend.title.pch = 'Exposure')    # 2 Plot the samples

plot(MyResult.pca)
MyResult.pca

#Show top 100 genes and their name
plotLoadings(MyResult.pca, ndisplay = 100, 
             name.var = liver.toxicity$gene.ID[, "geneBank"],
             size.name = rel(0.3))

#Show variables contributing to PC1/2/3 by top 15/10/5 genes from each PC
MyResult.spca <- spca(X, ncomp = 3, keepX = c(15,10,5))                 # 1 Run the method
plotIndiv(MyResult.spca, group = liver.toxicity$treatment$Dose.Group,   # 2 Plot the samples
          pch = as.factor(liver.toxicity$treatment$Time.Group),
          legend = TRUE, title = 'Liver toxicity: genes, sPCA comp 1 - 2',
          legend.title = 'Dose', legend.title.pch = 'Exposure')
plotVar(MyResult.spca, cex = 1)                                        # 3 Plot the variables, cex reduces size of labels

#Check weights used to define component
selectVar(MyResult.spca, comp = 1)$value
plotLoadings(MyResult.spca)
selectVar(MyResult.spca, comp=2)$value
plotLoadings(MyResult.spca, comp = 2)
```

## Partial Least Squares - Discriminant Analysis (PLS-DA) <supervised>

Purpose: To classify samples into known groups and predict classes of new samples. Identifying key variables that drive discrimination. Unlike PCA, the aim to to maximize covariance between x and y. 

```{r}
library(mixOmics)
data(srbct)
X <- srbct$gene
Y <- srbct$class 
summary(Y)

#Select 50 of each component for testing purposes
MyResult.splsda <- splsda(X, Y, keepX = c(50,50)) # 1 Run the method
plotIndiv(MyResult.splsda)                          # 2 Plot the samples
plotVar(MyResult.splsda)                            # 3 Plot the variables
selectVar(MyResult.splsda, comp=1)$name             # Selected variables on component 1

#Replace with icons
plotIndiv(MyResult.splsda, ind.names = FALSE, legend=TRUE,
          ellipse = TRUE, star = TRUE, title = 'sPLS-DA on SRBCT',
          X.label = 'PLS-DA 1', Y.label = 'PLS-DA 2')

#Add background gradient
background <- background.predict(MyResult.splsda, comp.predicted=2,
                                dist = "max.dist") 
plotIndiv(MyResult.splsda, comp = 1:2, group = srbct$class,
          ind.names = FALSE, title = "Maximum distance",
          legend = TRUE,  background = background)

#Contribution to component
MyResult.splsda2 <- splsda(X,Y, ncomp=3, keepX=c(15,10,5))
selectVar(MyResult.splsda2, comp=1)$value
plotLoadings(MyResult.splsda2, contrib = 'max', method = 'mean')
```

## Data Integration Analysis for Biomarker discovery using a Latent Components (DIABLO)

Purpose: Extension of Generalized Canonical Correlation Analysis. Identifies variables across related datasets to explain outcome of interest. Main outputs include:
1) Set of components - latent variables associated with each dataset
2) Set of loading vectors - coefficients assigned to each variable to define each component. Values indicate importance of each variable. Maximization of covariance between x and y.
3) List of selected variables - list of variables associated with component with sparse DIABLO

```{r}
#Load data
library(mixOmics)
data(breast.TCGA)
# extract training data and name each data frame
X <- list(mRNA = breast.TCGA$data.train$mrna, 
          miRNA = breast.TCGA$data.train$mirna, 
          protein = breast.TCGA$data.train$protein)
Y <- breast.TCGA$data.train$subtype
summary(Y)

#Select number of variables to keep in each dataset and each component
list.keepX <- list(mRNA = c(16, 17), miRNA = c(18,5), protein = c(5, 5))

#Generate splsda for each dataset (default: ncomp = 2, scale = TRUE meaning var = 1)
MyResult.diablo <- block.splsda(X, Y, keepX=list.keepX)
plotIndiv(MyResult.diablo)
plotVar(MyResult.diablo)

#Non sparse version
MyResult.diablo2 <- block.plsda(X, Y)

#Plot BRCA for each sub dataset
plotIndiv(MyResult.diablo, 
          ind.names = FALSE, 
          legend=TRUE, cex=c(1,2,3),
          title = 'BRCA with DIABLO')

plotVar(MyResult.diablo, var.names = c(FALSE, FALSE, TRUE),
        legend=TRUE, pch=c(16,16,1))

#Show correlation between datasets
plotDiablo(MyResult.diablo, ncomp = 1)

#Show correlation between proteome, transcriptome of each dataset
circosPlot(MyResult.diablo, cutoff=0.7)

#Show loading weights of each selected variable on each component and dataset
#plotLoadings(MyResult.diablo, contrib = "max")
plotLoadings(MyResult.diablo, comp = 2, contrib = "max")

#Use cross-validation with perf to check prediction of model
set.seed(123) # for reproducibility in this vignette
MyPerf.diablo <- perf(MyResult.diablo, validation = 'Mfold', folds = 5, 
                   nrepeat = 10, 
                   dist = 'centroids.dist')
```

## Definitions

Variable - unknown quantity to be studied
Random Variable - variable where value results from a quantitative measurement that is subject to variation
Probability - number assigned to an outcome to express chance of occurrence
Probability distribution - mathematical function that shows outcomes/events to probabilities
False positive rate (alpha) - total probability that test stat will fall into this rejection region even if null hypothesis is true
Degrees of freedom - number of independent values that can vary in an analysis, but the last number is concrete. E.g. if we had three unknown numbers to make an average of 5, two of those numbers can be anything, but the last one is constrained. For a two-sided test, it would be n1 + n2 - 2 (total samples for each, minus one constraint for each)
Simple/Conditional effects - effects at a given level of the other factor (e.g. effect of genotype at E16 timepoint). Cell mean differences between conditions of one level at a specific level of the other factor.
Marginal/Main effects - overall effect of factor averaged over all levels of the other factor (e.g. overall effect of genotype, averaged across all developmental timepoints E16 and P28). Measures the overall association between response and factor, weighted average over all levels of the other factor.

## Links
https://www.bioconductor.org/packages/devel/bioc/vignettes/mixOmics/inst/doc/vignette.html#diablo

